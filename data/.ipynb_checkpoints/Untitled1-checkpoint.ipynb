{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4590da-45e6-49e2-ab9a-1c2a6fda9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86cbd563-1efd-4be7-9aff-e94c5ea5fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 커스텀 BayesianLSTM 셀 구현\n",
    "class BayesianLSTMCell(tf.keras.layers.LSTMCell):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BayesianLSTMCell, self).__init__(units, **kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn()\n",
    "        self.kernel_prior_fn = lambda dtype, shape, name, trainable, add_variable_fn: tfp.layers.default_multivariate_normal_fn(\n",
    "            dtype=dtype, shape=shape, name=name, trainable=trainable, add_variable_fn=add_variable_fn)\n",
    "        self.recurrent_kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn()\n",
    "        self.recurrent_kernel_prior_fn = lambda dtype, shape, name, trainable, add_variable_fn: tfp.layers.default_multivariate_normal_fn(\n",
    "            dtype=dtype, shape=shape, name=name, trainable=trainable, add_variable_fn=add_variable_fn)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_posterior = self.kernel_posterior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[input_shape[-1], self.units * 4],\n",
    "            name='kernel_posterior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.kernel_prior = self.kernel_prior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[input_shape[-1], self.units * 4],\n",
    "            name='kernel_prior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.recurrent_kernel_posterior = self.recurrent_kernel_posterior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self.units, self.units * 4],\n",
    "            name='recurrent_kernel_posterior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.recurrent_kernel_prior = self.recurrent_kernel_prior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self.units, self.units * 4],\n",
    "            name='recurrent_kernel_prior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        super(BayesianLSTMCell, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        self.kernel = self.kernel_posterior.sample()\n",
    "        self.recurrent_kernel = self.recurrent_kernel_posterior.sample()\n",
    "        return super(BayesianLSTMCell, self).call(inputs, states, training=training)\n",
    "\n",
    "def build_bayesian_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        tf.keras.layers.RNN(BayesianLSTMCell(10)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    # model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    # return model\n",
    "    kl_divergence = sum(model.losses)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                  loss=lambda y_true, \n",
    "                  y_pred: tf.keras.losses.mean_squared_error(y_true, y_pred) + kl_divergence)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c40d8a-34c2-4690-86f9-289a18522a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_prepare_data(data_file):\n",
    "    \"\"\"\n",
    "    주어진 CSV 파일을 불러와 기본 전처리를 수행.\n",
    "    \n",
    "    매개변수:\n",
    "    - data_file: CSV 파일의 경로\n",
    "    \n",
    "    Returns:\n",
    "    - data: 전처리된 데이터프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "    # Prepare the data\n",
    "    data['Time'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "\n",
    "    data['Target_MHC_Water_Level'] = data['MHC_Water_Level'].shift(-3)\n",
    "\n",
    "    # Fill NaN values in 'Target_MHC_Water_Level' with the last value of 'MHC_Water_Level'\n",
    "    data['Target_MHC_Water_Level'].fillna(data['MHC_Water_Level'].iloc[-1], inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Add lag variables for MHC_Water_Level from t-1 to t-10\n",
    "    # for i in range(3, 11):\n",
    "    #     data[f'MHC_Water_Level_lag_{i}'] = data['MHC_Water_Level'].shift(i)\n",
    "    # data.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "    #Add lag variables for MHC_Water_Level from t-1 to t-10\n",
    "    for i in range(3, 6):\n",
    "        data[f'MHC_Water_Level_lag_{i}'] = data['MHC_Water_Level'].shift(i)\n",
    "        data[f'MH_Water_Level_lag_{i}'] = data['MH_Water_Level'].shift(i)\n",
    "        data[f'PG_Water_Level_lag_{i}'] = data['PG_Water_Level'].shift(i)\n",
    "        data[f'HH_Water_Level_lag_{i}'] = data['HH_Water_Level'].shift(i)\n",
    "        data[f'GG_Water_Level_lag_{i}'] = data['GG_Water_Level'].shift(i)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_data(data):\n",
    "    \"\"\"\n",
    "    데이터를 학습 및 테스트 데이터로 분할.\n",
    "    \n",
    "    매개변수:\n",
    "    - data: 전체 데이터프레임\n",
    "    \n",
    "    Returns:\n",
    "    - train_data: 학습 데이터\n",
    "    - test_data: 테스트 데이터\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    test_data = data.iloc[train_size:]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ed5aaa-0325-4907-8577-4fdf3b2d5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bayes_prepare_train_test_sets(train_data, test_data):\n",
    "    X_train = train_data.drop(columns=['Time', 'Target_MHC_Water_Level']).values\n",
    "    y_train = train_data['Target_MHC_Water_Level'].values\n",
    "    X_test = test_data.drop(columns=['Time', 'Target_MHC_Water_Level']).values\n",
    "    y_test = test_data['Target_MHC_Water_Level'].values\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f323e3eb-2caa-4d6e-bcdb-977438ba76ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 12:15:53.077074: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.077247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.077386: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.081192: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.081273: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.081354: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.081382: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-08-12 12:15:53.081857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/idhkdni/.local/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "501/501 [==============================] - 6s 10ms/step - loss: 9.1897e-04\n",
      "Epoch 2/10\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 6.4493e-04\n",
      "Epoch 3/10\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 6.1359e-04\n",
      "Epoch 4/10\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 5.5442e-04\n",
      "Epoch 5/10\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 5.3998e-04\n",
      "Epoch 6/10\n",
      "501/501 [==============================] - 5s 9ms/step - loss: 5.1617e-04\n",
      "Epoch 7/10\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 5.0629e-04\n",
      "Epoch 8/10\n",
      "501/501 [==============================] - 5s 9ms/step - loss: 4.9381e-04\n",
      "Epoch 9/10\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 4.7908e-04\n",
      "Epoch 10/10\n",
      "501/501 [==============================] - 5s 9ms/step - loss: 4.2047e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff988366410>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "                data = load_and_prepare_data('water_data.csv')\n",
    "                train_data, test_data = split_data(data)\n",
    "                X_train, y_train, X_test, y_test = bayes_prepare_train_test_sets(train_data, test_data)\n",
    "                \n",
    "                \n",
    "                scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "                scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "                X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "                X_test_scaled = scaler_X.transform(X_test)\n",
    "                y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "                \n",
    "                model = build_bayesian_lstm_model(input_shape=(X_train_scaled.shape[1], 1))\n",
    "                X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "                X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "                model.fit(X_train_scaled, y_train_scaled, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519bdae-fff6-4481-b1d7-ee74b11eb4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fe604-0bbc-4a84-be24-294020beda14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811164a5-c79c-4a82-8db1-b1410ffd72b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f2beac-e86e-4c67-ac2a-25d58c9546da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_pred_uncer =pd.read_csv('bayes_pred_uncer.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "936fb9ac-9787-450c-84c6-e56dc6603df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bayesian_visual_predictions_to_csv(model, \n",
    "                                            x,\n",
    "                                            time_points, \n",
    "                                            scaler_y,\n",
    "                                            n_iter=100):\n",
    "    \"\"\"\n",
    "    예측 결과를 CSV 파일로 저장하는 함수\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 학습된 Bayesian LSTM 모델\n",
    "    - x: 테스트 데이터의 특성 (정규화된 상태)\n",
    "    - time_points: 시간대 포인트 (원래 시간 데이터)\n",
    "    - scaler_y: 타겟 변수의 스케일러\n",
    "    - n_iter: 예측 수행 횟수\n",
    "    - filename: 저장할 CSV 파일 이름\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # time_points = time_points[-3:]\n",
    "    # print(time_points)\n",
    "    \n",
    "    predictions_scaled = np.zeros((len(time_points), n_iter))\n",
    "    \n",
    "    for j in range(n_iter):\n",
    "        predictions_scaled[:, j] = model(x, training=True).numpy().flatten()\n",
    "    \n",
    "    # 예측 결과 역정규화\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    # 시간대와 예측 결과를 DataFrame으로 변환\n",
    "    df_predictions = pd.DataFrame(\n",
    "                                  predictions, \n",
    "                                  columns=[f'Prediction_{i+1}' for i in range(n_iter)])\n",
    "    df_predictions.insert(0, 'Time', time_points)\n",
    "\n",
    "    \n",
    "    return df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdbc08b6-b096-47d7-a97c-b056072cea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 61.48241329193115 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "time_points = bayes_pred_uncer['Time']\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 실행할 코드\n",
    "bayes_ppd_data = save_bayesian_visual_predictions_to_csv(model, \n",
    "                                                         X_test_scaled,\n",
    "                                                         time_points, \n",
    "                                                         scaler_y,\n",
    "                                                         n_iter=300)\n",
    "\n",
    "# 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 경과 시간 계산\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27a88495-4bbf-4ac6-8f10-d7b356f8aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ppd_data = bayes_ppd_data.assign(Time=time_points.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec73d6e-2b5b-47df-8550-c14f943f9e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d41fd-70c3-433e-b04d-1a8aaadd6543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5b4747d-54d3-4ec0-8dfc-94acee18de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# @tf.function\n",
    "def predict_and_save_bayesian_visual_predictions(model, \n",
    "                                                 x, \n",
    "                                                 time_points, \n",
    "                                                 scaler_y, \n",
    "                                                 true_value,\n",
    "                                                 n_iter=5000):\n",
    "    \"\"\"\n",
    "    Bayesian LSTM 모델의 예측 결과를 여러 번 수행하여 불확실성과 신뢰 구간을 포함한\n",
    "    예측 분포를 계산하고 이를 DataFrame으로 반환하는 함수.\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 학습된 Bayesian LSTM 모델\n",
    "    - x: 테스트 데이터의 특성 (정규화된 상태)\n",
    "    - time_points: 시간대 포인트 (원래 시간 데이터)\n",
    "    - scaler_y: 타겟 변수의 스케일러\n",
    "    - n_iter: 예측 수행 횟수\n",
    "    \n",
    "    반환값:\n",
    "    - df_predictions: 예측 결과와 신뢰 구간을 포함한 DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # 여러 번의 예측 수행\n",
    "    result = tf.TensorArray(tf.float32, size=n_iter)\n",
    "    for i in tf.range(n_iter):\n",
    "        result = result.write(i, model(x, training=True))\n",
    "    result = result.stack()  # (n_iter, batch_size, output_size)\n",
    "    \n",
    "    # 평균 예측 값 계산\n",
    "    prediction = tf.reduce_mean(result, axis=0)\n",
    "    \n",
    "    # 불확실성 계산 (표준편차)\n",
    "    uncertainty = tf.math.reduce_std(result, axis=0)\n",
    "    \n",
    "    # 95% 신뢰구간 (credible interval) 계산\n",
    "    lower_bound = tfp.stats.percentile(result, 2.5, axis=0)\n",
    "    upper_bound = tfp.stats.percentile(result, 97.5, axis=0)\n",
    "    \n",
    "    # 예측 결과 역정규화\n",
    "    prediction = scaler_y.inverse_transform(prediction.numpy())\n",
    "    lower_bound = scaler_y.inverse_transform(lower_bound.numpy())\n",
    "    upper_bound = scaler_y.inverse_transform(upper_bound.numpy())\n",
    "    \n",
    "    # 각 예측을 역정규화하고 DataFrame에 포함시키기 위해 loop\n",
    "    predictions_scaled = np.zeros((len(time_points), n_iter))\n",
    "    for j in range(n_iter):\n",
    "        predictions_scaled[:, j] = scaler_y.inverse_transform(result[j].numpy().flatten().reshape(-1, 1)).flatten()\n",
    "\n",
    "    # 시간대와 예측 결과를 DataFrame으로 변환\n",
    "    df_predictions = pd.DataFrame(predictions_scaled, \n",
    "                                  columns=[f'Prediction_{i+1}' for i in range(n_iter)])\n",
    "    df_predictions.insert(0, 'Time', time_points)\n",
    "    df_predictions['Lower_Bound'] = lower_bound.flatten()\n",
    "    df_predictions['Upper_Bound'] = upper_bound.flatten()\n",
    "    df_predictions['Mean_Prediction'] = prediction.flatten()\n",
    "    df_predictions['Uncertainty'] = uncertainty.numpy().flatten()\n",
    "    df_predictions['True_value'] = true_value\n",
    "\n",
    "    \n",
    "    # 열 순서 변경: Mean_Prediction, Uncertainty, Lower_Bound, Upper_Bound, Time + 나머지 Prediction 열\n",
    "    cols = ['Time', 'True_value', 'Mean_Prediction', 'Uncertainty', 'Lower_Bound', 'Upper_Bound'] + [col for col in df_predictions.columns if col.startswith('Prediction_')]\n",
    "    df_predictions = df_predictions[cols]\n",
    "    \n",
    "    return df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3eb301-443e-4c08-827d-f88cc89f6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def predict_and_save_bayesian_visual_predictions(model, \n",
    "                                                 x, \n",
    "                                                 time_points, \n",
    "                                                 scaler_y, \n",
    "                                                 true_value,\n",
    "                                                 n_iter=50):\n",
    "    \"\"\"\n",
    "    Bayesian LSTM 모델의 예측 결과를 여러 번 수행하여 불확실성과 신뢰 구간을 포함한\n",
    "    예측 분포를 계산하고 이를 DataFrame으로 반환하는 함수.\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 학습된 Bayesian LSTM 모델\n",
    "    - x: 테스트 데이터의 특성 (정규화된 상태)\n",
    "    - time_points: 시간대 포인트 (원래 시간 데이터)\n",
    "    - scaler_y: 타겟 변수의 스케일러\n",
    "    - n_iter: 예측 수행 횟수\n",
    "    \n",
    "    반환값:\n",
    "    - df_predictions: 예측 결과와 신뢰 구간을 포함한 DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # 여러 번의 예측 수행\n",
    "    result = tf.TensorArray(tf.float32, size=n_iter)\n",
    "    for i in tqdm(range(n_iter), desc=\"Predictions\"):\n",
    "        result = result.write(i, model(x, training=True))\n",
    "    result = result.stack()  # (n_iter, batch_size, output_size)\n",
    "    \n",
    "    # 평균 예측 값 계산\n",
    "    prediction = tf.reduce_mean(result, axis=0)\n",
    "    \n",
    "    # 불확실성 계산 (표준편차)\n",
    "    uncertainty = tf.math.reduce_std(result, axis=0)\n",
    "    \n",
    "    # 95% 신뢰구간 (credible interval) 계산\n",
    "    lower_bound = tfp.stats.percentile(result, 2.5, axis=0)\n",
    "    upper_bound = tfp.stats.percentile(result, 97.5, axis=0)\n",
    "    \n",
    "    # 예측 결과 역정규화\n",
    "    prediction = scaler_y.inverse_transform(prediction.numpy())\n",
    "    lower_bound = scaler_y.inverse_transform(lower_bound.numpy())\n",
    "    upper_bound = scaler_y.inverse_transform(upper_bound.numpy())\n",
    "    \n",
    "    # 각 예측을 역정규화하고 DataFrame에 포함시키기 위해 loop\n",
    "    predictions_scaled = np.zeros((len(time_points), n_iter))\n",
    "    for j in range(n_iter):\n",
    "        predictions_scaled[:, j] = scaler_y.inverse_transform(result[j].numpy().flatten().reshape(-1, 1)).flatten()\n",
    "\n",
    "    # 시간대와 예측 결과를 DataFrame으로 변환\n",
    "    df_predictions = pd.DataFrame(predictions_scaled, \n",
    "                                  columns=[f'Prediction_{i+1}' for i in range(n_iter)])\n",
    "    df_predictions.insert(0, 'Time', time_points)\n",
    "    df_predictions['Lower_Bound'] = lower_bound.flatten()\n",
    "    df_predictions['Upper_Bound'] = upper_bound.flatten()\n",
    "    df_predictions['Mean_Prediction'] = prediction.flatten()\n",
    "    df_predictions['Uncertainty'] = uncertainty.numpy().flatten()\n",
    "    df_predictions['True_Value'] = true_value\n",
    "    \n",
    "    # 열 순서 변경: Mean_Prediction, Uncertainty, Lower_Bound, Upper_Bound, Time + 나머지 Prediction 열\n",
    "    cols = ['Time', 'True_Value', 'Mean_Prediction', 'Uncertainty', 'Lower_Bound', 'Upper_Bound'] + [col for col in df_predictions.columns if col.startswith('Prediction_')]\n",
    "    df_predictions = df_predictions[cols]\n",
    "    \n",
    "    return df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51a7cf66-54a1-43d5-8f7f-89e36bda4388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16022    1.50\n",
       "16023    1.49\n",
       "16024    1.50\n",
       "16025    1.49\n",
       "16026    1.49\n",
       "         ... \n",
       "20022    1.51\n",
       "20023    1.50\n",
       "20024    1.50\n",
       "20025    1.50\n",
       "20026    1.49\n",
       "Name: MHC_Water_Level, Length: 4005, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['MHC_Water_Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a8e37a7-931d-4eb2-a07d-eb84b1c0cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 불확실성 계산, Bayesian PPD 데이터 생성\n",
    "time_points = test_data[\"Time\"] + pd.Timedelta(hours=3)\n",
    "true_value = test_data['MHC_Water_Level']\n",
    "bayes_ppd_data = predict_and_save_bayesian_visual_predictions(model, \n",
    "                                                              X_test_scaled, \n",
    "                                                              time_points,\n",
    "                                                              scaler_y,\n",
    "                                                              true_value, \n",
    "                                                              n_iter=50)\n",
    "\n",
    "# # 예측 데이터 정리 및 저장\n",
    "# bayes_pred_uncer = pd.DataFrame({\n",
    "#     'Time': time_points,\n",
    "#     \"True_Value\": test_data['MHC_Water_Level'].shift(-3),\n",
    "#     \"Prediction\": bayes_ppd_data['Mean_Prediction'],\n",
    "#     \"Uncertainty\": bayes_ppd_data['Uncertainty']\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c03f79e-f289-4418-909a-c81c6b8dbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ppd_data = bayes_ppd_data.assign(Time=time_points.values)\n",
    "bayes_ppd_data = bayes_ppd_data.assign(True_value=true_value.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4828189-1c79-40cf-b3cd-576203935009",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ppd_data['True_value'] = bayes_ppd_data['True_value'].shift(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72c2016f-0281-4ad1-be35-283e2708cfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>True_value</th>\n",
       "      <th>Mean_Prediction</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Lower_Bound</th>\n",
       "      <th>Upper_Bound</th>\n",
       "      <th>Prediction_1</th>\n",
       "      <th>Prediction_2</th>\n",
       "      <th>Prediction_3</th>\n",
       "      <th>Prediction_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Prediction_41</th>\n",
       "      <th>Prediction_42</th>\n",
       "      <th>Prediction_43</th>\n",
       "      <th>Prediction_44</th>\n",
       "      <th>Prediction_45</th>\n",
       "      <th>Prediction_46</th>\n",
       "      <th>Prediction_47</th>\n",
       "      <th>Prediction_48</th>\n",
       "      <th>Prediction_49</th>\n",
       "      <th>Prediction_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-27 18:00:00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.533030</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>1.509882</td>\n",
       "      <td>1.559967</td>\n",
       "      <td>1.531343</td>\n",
       "      <td>1.529574</td>\n",
       "      <td>1.515125</td>\n",
       "      <td>1.540432</td>\n",
       "      <td>...</td>\n",
       "      <td>1.539271</td>\n",
       "      <td>1.534396</td>\n",
       "      <td>1.512478</td>\n",
       "      <td>1.527979</td>\n",
       "      <td>1.529423</td>\n",
       "      <td>1.558028</td>\n",
       "      <td>1.555886</td>\n",
       "      <td>1.535634</td>\n",
       "      <td>1.522156</td>\n",
       "      <td>1.506619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-27 19:00:00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.526218</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>1.503145</td>\n",
       "      <td>1.553415</td>\n",
       "      <td>1.524679</td>\n",
       "      <td>1.522624</td>\n",
       "      <td>1.508402</td>\n",
       "      <td>1.533628</td>\n",
       "      <td>...</td>\n",
       "      <td>1.532550</td>\n",
       "      <td>1.527484</td>\n",
       "      <td>1.505556</td>\n",
       "      <td>1.521043</td>\n",
       "      <td>1.522524</td>\n",
       "      <td>1.551328</td>\n",
       "      <td>1.549004</td>\n",
       "      <td>1.528852</td>\n",
       "      <td>1.515445</td>\n",
       "      <td>1.499717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-27 20:00:00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.526531</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>1.503426</td>\n",
       "      <td>1.553564</td>\n",
       "      <td>1.524946</td>\n",
       "      <td>1.522968</td>\n",
       "      <td>1.508635</td>\n",
       "      <td>1.533849</td>\n",
       "      <td>...</td>\n",
       "      <td>1.532861</td>\n",
       "      <td>1.527820</td>\n",
       "      <td>1.505932</td>\n",
       "      <td>1.521429</td>\n",
       "      <td>1.522813</td>\n",
       "      <td>1.551645</td>\n",
       "      <td>1.549382</td>\n",
       "      <td>1.529125</td>\n",
       "      <td>1.515733</td>\n",
       "      <td>1.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-27 21:00:00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.524298</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>1.501251</td>\n",
       "      <td>1.551419</td>\n",
       "      <td>1.522817</td>\n",
       "      <td>1.520737</td>\n",
       "      <td>1.506380</td>\n",
       "      <td>1.531610</td>\n",
       "      <td>...</td>\n",
       "      <td>1.530683</td>\n",
       "      <td>1.525562</td>\n",
       "      <td>1.503582</td>\n",
       "      <td>1.519196</td>\n",
       "      <td>1.520543</td>\n",
       "      <td>1.549434</td>\n",
       "      <td>1.547167</td>\n",
       "      <td>1.526813</td>\n",
       "      <td>1.513484</td>\n",
       "      <td>1.497736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-27 22:00:00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.523542</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>1.500450</td>\n",
       "      <td>1.550647</td>\n",
       "      <td>1.522054</td>\n",
       "      <td>1.519937</td>\n",
       "      <td>1.505638</td>\n",
       "      <td>1.530846</td>\n",
       "      <td>...</td>\n",
       "      <td>1.529947</td>\n",
       "      <td>1.524819</td>\n",
       "      <td>1.502856</td>\n",
       "      <td>1.518415</td>\n",
       "      <td>1.519820</td>\n",
       "      <td>1.548699</td>\n",
       "      <td>1.546374</td>\n",
       "      <td>1.526080</td>\n",
       "      <td>1.512771</td>\n",
       "      <td>1.496940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>2024-08-12 10:00:00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.373650</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>1.349979</td>\n",
       "      <td>1.402773</td>\n",
       "      <td>1.370621</td>\n",
       "      <td>1.366626</td>\n",
       "      <td>1.362019</td>\n",
       "      <td>1.373457</td>\n",
       "      <td>...</td>\n",
       "      <td>1.389051</td>\n",
       "      <td>1.373958</td>\n",
       "      <td>1.349979</td>\n",
       "      <td>1.370590</td>\n",
       "      <td>1.367581</td>\n",
       "      <td>1.397506</td>\n",
       "      <td>1.388265</td>\n",
       "      <td>1.369187</td>\n",
       "      <td>1.365716</td>\n",
       "      <td>1.354544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>2024-08-12 11:00:00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.371779</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>1.348004</td>\n",
       "      <td>1.400941</td>\n",
       "      <td>1.368783</td>\n",
       "      <td>1.364775</td>\n",
       "      <td>1.360146</td>\n",
       "      <td>1.371593</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387215</td>\n",
       "      <td>1.372074</td>\n",
       "      <td>1.348004</td>\n",
       "      <td>1.368702</td>\n",
       "      <td>1.365663</td>\n",
       "      <td>1.395638</td>\n",
       "      <td>1.386446</td>\n",
       "      <td>1.367255</td>\n",
       "      <td>1.363875</td>\n",
       "      <td>1.352679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>2024-08-12 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.372071</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>1.348276</td>\n",
       "      <td>1.401292</td>\n",
       "      <td>1.369061</td>\n",
       "      <td>1.365165</td>\n",
       "      <td>1.360495</td>\n",
       "      <td>1.371904</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387550</td>\n",
       "      <td>1.372390</td>\n",
       "      <td>1.348276</td>\n",
       "      <td>1.369012</td>\n",
       "      <td>1.365946</td>\n",
       "      <td>1.395915</td>\n",
       "      <td>1.386675</td>\n",
       "      <td>1.367525</td>\n",
       "      <td>1.364121</td>\n",
       "      <td>1.353006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>2024-08-12 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.370692</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>1.346948</td>\n",
       "      <td>1.399895</td>\n",
       "      <td>1.367728</td>\n",
       "      <td>1.363734</td>\n",
       "      <td>1.359066</td>\n",
       "      <td>1.370491</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386147</td>\n",
       "      <td>1.370970</td>\n",
       "      <td>1.346948</td>\n",
       "      <td>1.367607</td>\n",
       "      <td>1.364627</td>\n",
       "      <td>1.394517</td>\n",
       "      <td>1.385329</td>\n",
       "      <td>1.366164</td>\n",
       "      <td>1.362802</td>\n",
       "      <td>1.351601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>2024-08-12 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.363637</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>1.339712</td>\n",
       "      <td>1.392962</td>\n",
       "      <td>1.360785</td>\n",
       "      <td>1.356679</td>\n",
       "      <td>1.351990</td>\n",
       "      <td>1.363383</td>\n",
       "      <td>...</td>\n",
       "      <td>1.379214</td>\n",
       "      <td>1.363837</td>\n",
       "      <td>1.339712</td>\n",
       "      <td>1.360430</td>\n",
       "      <td>1.357574</td>\n",
       "      <td>1.387535</td>\n",
       "      <td>1.378285</td>\n",
       "      <td>1.359016</td>\n",
       "      <td>1.355832</td>\n",
       "      <td>1.344489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4005 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time  True_value  Mean_Prediction  Uncertainty  \\\n",
       "0    2024-02-27 18:00:00        1.49         1.533030     0.001491   \n",
       "1    2024-02-27 19:00:00        1.49         1.526218     0.001494   \n",
       "2    2024-02-27 20:00:00        1.49         1.526531     0.001494   \n",
       "3    2024-02-27 21:00:00        1.49         1.524298     0.001495   \n",
       "4    2024-02-27 22:00:00        1.49         1.523542     0.001495   \n",
       "...                  ...         ...              ...          ...   \n",
       "4000 2024-08-12 10:00:00        1.50         1.373650     0.001406   \n",
       "4001 2024-08-12 11:00:00        1.49         1.371779     0.001407   \n",
       "4002 2024-08-12 12:00:00         NaN         1.372071     0.001407   \n",
       "4003 2024-08-12 13:00:00         NaN         1.370692     0.001406   \n",
       "4004 2024-08-12 14:00:00         NaN         1.363637     0.001409   \n",
       "\n",
       "      Lower_Bound  Upper_Bound  Prediction_1  Prediction_2  Prediction_3  \\\n",
       "0        1.509882     1.559967      1.531343      1.529574      1.515125   \n",
       "1        1.503145     1.553415      1.524679      1.522624      1.508402   \n",
       "2        1.503426     1.553564      1.524946      1.522968      1.508635   \n",
       "3        1.501251     1.551419      1.522817      1.520737      1.506380   \n",
       "4        1.500450     1.550647      1.522054      1.519937      1.505638   \n",
       "...           ...          ...           ...           ...           ...   \n",
       "4000     1.349979     1.402773      1.370621      1.366626      1.362019   \n",
       "4001     1.348004     1.400941      1.368783      1.364775      1.360146   \n",
       "4002     1.348276     1.401292      1.369061      1.365165      1.360495   \n",
       "4003     1.346948     1.399895      1.367728      1.363734      1.359066   \n",
       "4004     1.339712     1.392962      1.360785      1.356679      1.351990   \n",
       "\n",
       "      Prediction_4  ...  Prediction_41  Prediction_42  Prediction_43  \\\n",
       "0         1.540432  ...       1.539271       1.534396       1.512478   \n",
       "1         1.533628  ...       1.532550       1.527484       1.505556   \n",
       "2         1.533849  ...       1.532861       1.527820       1.505932   \n",
       "3         1.531610  ...       1.530683       1.525562       1.503582   \n",
       "4         1.530846  ...       1.529947       1.524819       1.502856   \n",
       "...            ...  ...            ...            ...            ...   \n",
       "4000      1.373457  ...       1.389051       1.373958       1.349979   \n",
       "4001      1.371593  ...       1.387215       1.372074       1.348004   \n",
       "4002      1.371904  ...       1.387550       1.372390       1.348276   \n",
       "4003      1.370491  ...       1.386147       1.370970       1.346948   \n",
       "4004      1.363383  ...       1.379214       1.363837       1.339712   \n",
       "\n",
       "      Prediction_44  Prediction_45  Prediction_46  Prediction_47  \\\n",
       "0          1.527979       1.529423       1.558028       1.555886   \n",
       "1          1.521043       1.522524       1.551328       1.549004   \n",
       "2          1.521429       1.522813       1.551645       1.549382   \n",
       "3          1.519196       1.520543       1.549434       1.547167   \n",
       "4          1.518415       1.519820       1.548699       1.546374   \n",
       "...             ...            ...            ...            ...   \n",
       "4000       1.370590       1.367581       1.397506       1.388265   \n",
       "4001       1.368702       1.365663       1.395638       1.386446   \n",
       "4002       1.369012       1.365946       1.395915       1.386675   \n",
       "4003       1.367607       1.364627       1.394517       1.385329   \n",
       "4004       1.360430       1.357574       1.387535       1.378285   \n",
       "\n",
       "      Prediction_48  Prediction_49  Prediction_50  \n",
       "0          1.535634       1.522156       1.506619  \n",
       "1          1.528852       1.515445       1.499717  \n",
       "2          1.529125       1.515733       1.500005  \n",
       "3          1.526813       1.513484       1.497736  \n",
       "4          1.526080       1.512771       1.496940  \n",
       "...             ...            ...            ...  \n",
       "4000       1.369187       1.365716       1.354544  \n",
       "4001       1.367255       1.363875       1.352679  \n",
       "4002       1.367525       1.364121       1.353006  \n",
       "4003       1.366164       1.362802       1.351601  \n",
       "4004       1.359016       1.355832       1.344489  \n",
       "\n",
       "[4005 rows x 56 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_ppd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4e744-b757-4a7e-9b5c-44e51e21b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CSV 파일로 저장\n",
    "# bayes_pred_uncer.to_csv('../streamlit/data/bayes_pred_uncer.csv', index=False)\n",
    "# bayes_ppd_data.to_csv('../streamlit/data/bayesian_ppd_visual.csv', index=False)\n",
    "\n",
    "# print('Bayesian LSTM Model Learning & Prediction Finish\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skku_dmlab [Python/conda:env]",
   "language": "python",
   "name": "conda-env-skku_dmlab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
