{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa4590da-45e6-49e2-ab9a-1c2a6fda9cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 22:43:28.289646: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 22:43:28.477667: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-11 22:43:28.477711: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-08-11 22:43:29.159955: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-11 22:43:29.160047: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-11 22:43:29.160057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86cbd563-1efd-4be7-9aff-e94c5ea5fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 커스텀 BayesianLSTM 셀 구현\n",
    "class BayesianLSTMCell(tf.keras.layers.LSTMCell):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BayesianLSTMCell, self).__init__(units, **kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn()\n",
    "        self.kernel_prior_fn = lambda dtype, shape, name, trainable, add_variable_fn: tfp.layers.default_multivariate_normal_fn(\n",
    "            dtype=dtype, shape=shape, name=name, trainable=trainable, add_variable_fn=add_variable_fn)\n",
    "        self.recurrent_kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn()\n",
    "        self.recurrent_kernel_prior_fn = lambda dtype, shape, name, trainable, add_variable_fn: tfp.layers.default_multivariate_normal_fn(\n",
    "            dtype=dtype, shape=shape, name=name, trainable=trainable, add_variable_fn=add_variable_fn)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_posterior = self.kernel_posterior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[input_shape[-1], self.units * 4],\n",
    "            name='kernel_posterior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.kernel_prior = self.kernel_prior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[input_shape[-1], self.units * 4],\n",
    "            name='kernel_prior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.recurrent_kernel_posterior = self.recurrent_kernel_posterior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self.units, self.units * 4],\n",
    "            name='recurrent_kernel_posterior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.recurrent_kernel_prior = self.recurrent_kernel_prior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self.units, self.units * 4],\n",
    "            name='recurrent_kernel_prior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        super(BayesianLSTMCell, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        self.kernel = self.kernel_posterior.sample()\n",
    "        self.recurrent_kernel = self.recurrent_kernel_posterior.sample()\n",
    "        return super(BayesianLSTMCell, self).call(inputs, states, training=training)\n",
    "\n",
    "def build_bayesian_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        tf.keras.layers.RNN(BayesianLSTMCell(10)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    # model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    # return model\n",
    "    kl_divergence = sum(model.losses)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                  loss=lambda y_true, \n",
    "                  y_pred: tf.keras.losses.mean_squared_error(y_true, y_pred) + kl_divergence)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c40d8a-34c2-4690-86f9-289a18522a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_prepare_data(data_file):\n",
    "    \"\"\"\n",
    "    주어진 CSV 파일을 불러와 기본 전처리를 수행.\n",
    "    \n",
    "    매개변수:\n",
    "    - data_file: CSV 파일의 경로\n",
    "    \n",
    "    Returns:\n",
    "    - data: 전처리된 데이터프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "    # Prepare the data\n",
    "    data['Time'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "\n",
    "    data['Target_MHC_Water_Level'] = data['MHC_Water_Level'].shift(-3)\n",
    "\n",
    "    # Fill NaN values in 'Target_MHC_Water_Level' with the last value of 'MHC_Water_Level'\n",
    "    data['Target_MHC_Water_Level'].fillna(data['MHC_Water_Level'].iloc[-1], inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Add lag variables for MHC_Water_Level from t-1 to t-10\n",
    "    # for i in range(3, 11):\n",
    "    #     data[f'MHC_Water_Level_lag_{i}'] = data['MHC_Water_Level'].shift(i)\n",
    "    # data.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "    #Add lag variables for MHC_Water_Level from t-1 to t-10\n",
    "    for i in range(3, 6):\n",
    "        data[f'MHC_Water_Level_lag_{i}'] = data['MHC_Water_Level'].shift(i)\n",
    "        data[f'MH_Water_Level_lag_{i}'] = data['MH_Water_Level'].shift(i)\n",
    "        data[f'PG_Water_Level_lag_{i}'] = data['PG_Water_Level'].shift(i)\n",
    "        data[f'HH_Water_Level_lag_{i}'] = data['HH_Water_Level'].shift(i)\n",
    "        data[f'GG_Water_Level_lag_{i}'] = data['GG_Water_Level'].shift(i)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_data(data):\n",
    "    \"\"\"\n",
    "    데이터를 학습 및 테스트 데이터로 분할.\n",
    "    \n",
    "    매개변수:\n",
    "    - data: 전체 데이터프레임\n",
    "    \n",
    "    Returns:\n",
    "    - train_data: 학습 데이터\n",
    "    - test_data: 테스트 데이터\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    test_data = data.iloc[train_size:]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ed5aaa-0325-4907-8577-4fdf3b2d5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bayes_prepare_train_test_sets(train_data, test_data):\n",
    "    X_train = train_data.drop(columns=['Time', 'Target_MHC_Water_Level']).values\n",
    "    y_train = train_data['Target_MHC_Water_Level'].values\n",
    "    X_test = test_data.drop(columns=['Time', 'Target_MHC_Water_Level']).values\n",
    "    y_test = test_data['Target_MHC_Water_Level'].values\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f323e3eb-2caa-4d6e-bcdb-977438ba76ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 22:43:37.542912: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-11 22:43:37.542958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2024-08-11 22:43:37.543271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /home/idhkdni/.local/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "501/501 [==============================] - 6s 9ms/step - loss: 0.0011\n",
      "Epoch 2/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 6.1140e-04\n",
      "Epoch 3/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 5.8809e-04\n",
      "Epoch 4/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 5.7359e-04\n",
      "Epoch 5/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 5.3064e-04\n",
      "Epoch 6/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 4.8430e-04\n",
      "Epoch 7/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 4.7703e-04\n",
      "Epoch 8/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 3.9768e-04\n",
      "Epoch 9/100\n",
      "501/501 [==============================] - 27s 54ms/step - loss: 3.6517e-04\n",
      "Epoch 10/100\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 3.7083e-04\n",
      "Epoch 11/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 3.3303e-04\n",
      "Epoch 12/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 3.0857e-04\n",
      "Epoch 13/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 3.0674e-04\n",
      "Epoch 14/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 2.7203e-04\n",
      "Epoch 15/100\n",
      "501/501 [==============================] - 19s 37ms/step - loss: 2.6841e-04\n",
      "Epoch 16/100\n",
      "501/501 [==============================] - 14s 27ms/step - loss: 2.5143e-04\n",
      "Epoch 17/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 2.3811e-04\n",
      "Epoch 18/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 2.2704e-04\n",
      "Epoch 19/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 2.3963e-04\n",
      "Epoch 20/100\n",
      "501/501 [==============================] - 8s 16ms/step - loss: 2.2813e-04\n",
      "Epoch 21/100\n",
      "501/501 [==============================] - 24s 49ms/step - loss: 2.3135e-04\n",
      "Epoch 22/100\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 1.9009e-04\n",
      "Epoch 23/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.8505e-04\n",
      "Epoch 24/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.7341e-04\n",
      "Epoch 25/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.6548e-04\n",
      "Epoch 26/100\n",
      "501/501 [==============================] - 19s 37ms/step - loss: 1.6802e-04\n",
      "Epoch 27/100\n",
      "501/501 [==============================] - 13s 26ms/step - loss: 1.5551e-04\n",
      "Epoch 28/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.6466e-04\n",
      "Epoch 29/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.5682e-04\n",
      "Epoch 30/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.5717e-04\n",
      "Epoch 31/100\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 1.5267e-04\n",
      "Epoch 32/100\n",
      "501/501 [==============================] - 35s 70ms/step - loss: 1.6106e-04\n",
      "Epoch 33/100\n",
      "501/501 [==============================] - 36s 72ms/step - loss: 1.5233e-04\n",
      "Epoch 34/100\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 1.5617e-04\n",
      "Epoch 35/100\n",
      "501/501 [==============================] - 30s 61ms/step - loss: 1.5000e-04\n",
      "Epoch 36/100\n",
      "501/501 [==============================] - 6s 13ms/step - loss: 1.4725e-04\n",
      "Epoch 37/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.4855e-04\n",
      "Epoch 38/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.4783e-04\n",
      "Epoch 39/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.4475e-04\n",
      "Epoch 40/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.3643e-04\n",
      "Epoch 41/100\n",
      "501/501 [==============================] - 31s 62ms/step - loss: 1.3888e-04\n",
      "Epoch 42/100\n",
      "501/501 [==============================] - 6s 12ms/step - loss: 1.3971e-04\n",
      "Epoch 43/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.4607e-04\n",
      "Epoch 44/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.4221e-04\n",
      "Epoch 45/100\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 1.3652e-04\n",
      "Epoch 46/100\n",
      "501/501 [==============================] - 15s 30ms/step - loss: 1.3257e-04\n",
      "Epoch 47/100\n",
      "501/501 [==============================] - 21s 43ms/step - loss: 1.3295e-04\n",
      "Epoch 48/100\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 1.3202e-04\n",
      "Epoch 49/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.4500e-04\n",
      "Epoch 50/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2975e-04\n",
      "Epoch 51/100\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 1.3533e-04\n",
      "Epoch 52/100\n",
      "501/501 [==============================] - 24s 49ms/step - loss: 1.3507e-04\n",
      "Epoch 53/100\n",
      "501/501 [==============================] - 12s 24ms/step - loss: 1.3163e-04\n",
      "Epoch 54/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.3784e-04\n",
      "Epoch 55/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.3358e-04\n",
      "Epoch 56/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.3888e-04\n",
      "Epoch 57/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.3261e-04\n",
      "Epoch 58/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.3215e-04\n",
      "Epoch 59/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.3076e-04\n",
      "Epoch 60/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.2874e-04\n",
      "Epoch 61/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.3232e-04\n",
      "Epoch 62/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.3351e-04\n",
      "Epoch 63/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2683e-04\n",
      "Epoch 64/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2915e-04\n",
      "Epoch 65/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.3616e-04\n",
      "Epoch 66/100\n",
      "501/501 [==============================] - 3s 7ms/step - loss: 1.2793e-04\n",
      "Epoch 67/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.3101e-04\n",
      "Epoch 68/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.3127e-04\n",
      "Epoch 69/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2893e-04\n",
      "Epoch 70/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.2054e-04\n",
      "Epoch 71/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.3597e-04\n",
      "Epoch 72/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.2512e-04\n",
      "Epoch 73/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.2856e-04\n",
      "Epoch 74/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2048e-04\n",
      "Epoch 75/100\n",
      "501/501 [==============================] - 3s 7ms/step - loss: 1.2653e-04\n",
      "Epoch 76/100\n",
      "501/501 [==============================] - 3s 7ms/step - loss: 1.2296e-04\n",
      "Epoch 77/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2609e-04\n",
      "Epoch 78/100\n",
      "501/501 [==============================] - 3s 7ms/step - loss: 1.2558e-04\n",
      "Epoch 79/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2748e-04\n",
      "Epoch 80/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2433e-04\n",
      "Epoch 81/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.2476e-04\n",
      "Epoch 82/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2916e-04\n",
      "Epoch 83/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2233e-04\n",
      "Epoch 84/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.1878e-04\n",
      "Epoch 85/100\n",
      "501/501 [==============================] - 3s 7ms/step - loss: 1.2266e-04\n",
      "Epoch 86/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.1871e-04\n",
      "Epoch 87/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2070e-04\n",
      "Epoch 88/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.2335e-04\n",
      "Epoch 89/100\n",
      "501/501 [==============================] - 4s 8ms/step - loss: 1.2606e-04\n",
      "Epoch 90/100\n",
      "501/501 [==============================] - 9s 17ms/step - loss: 1.1935e-04\n",
      "Epoch 91/100\n",
      "501/501 [==============================] - 27s 54ms/step - loss: 1.2016e-04\n",
      "Epoch 92/100\n",
      "501/501 [==============================] - 5s 11ms/step - loss: 1.1783e-04\n",
      "Epoch 93/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.2246e-04\n",
      "Epoch 94/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.1726e-04\n",
      "Epoch 95/100\n",
      "501/501 [==============================] - 3s 7ms/step - loss: 1.1771e-04\n",
      "Epoch 96/100\n",
      "501/501 [==============================] - 3s 6ms/step - loss: 1.1211e-04\n",
      "Epoch 97/100\n",
      "501/501 [==============================] - 6s 13ms/step - loss: 1.2001e-04\n",
      "Epoch 98/100\n",
      "501/501 [==============================] - 6s 13ms/step - loss: 1.1904e-04\n",
      "Epoch 99/100\n",
      "501/501 [==============================] - 5s 9ms/step - loss: 1.1847e-04\n",
      "Epoch 100/100\n",
      "501/501 [==============================] - 4s 7ms/step - loss: 1.1304e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d087f7c90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "                data = load_and_prepare_data('water_data.csv')\n",
    "                train_data, test_data = split_data(data)\n",
    "                X_train, y_train, X_test, y_test = bayes_prepare_train_test_sets(train_data, test_data)\n",
    "                \n",
    "                \n",
    "                scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "                scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "                X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "                X_test_scaled = scaler_X.transform(X_test)\n",
    "                y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "                \n",
    "                model = build_bayesian_lstm_model(input_shape=(X_train_scaled.shape[1], 1))\n",
    "                X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "                X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "                model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f2beac-e86e-4c67-ac2a-25d58c9546da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_pred_uncer =pd.read_csv('bayes_pred_uncer.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "936fb9ac-9787-450c-84c6-e56dc6603df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bayesian_visual_predictions_to_csv(model, \n",
    "                                            x,\n",
    "                                            time_points, \n",
    "                                            scaler_y,\n",
    "                                            n_iter=100):\n",
    "    \"\"\"\n",
    "    예측 결과를 CSV 파일로 저장하는 함수\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 학습된 Bayesian LSTM 모델\n",
    "    - x: 테스트 데이터의 특성 (정규화된 상태)\n",
    "    - time_points: 시간대 포인트 (원래 시간 데이터)\n",
    "    - scaler_y: 타겟 변수의 스케일러\n",
    "    - n_iter: 예측 수행 횟수\n",
    "    - filename: 저장할 CSV 파일 이름\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # time_points = time_points[-3:]\n",
    "    # print(time_points)\n",
    "    \n",
    "    predictions_scaled = np.zeros((len(time_points), n_iter))\n",
    "    \n",
    "    for j in range(n_iter):\n",
    "        predictions_scaled[:, j] = model(x, training=True).numpy().flatten()\n",
    "    \n",
    "    # 예측 결과 역정규화\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    # 시간대와 예측 결과를 DataFrame으로 변환\n",
    "    df_predictions = pd.DataFrame(\n",
    "                                  predictions, \n",
    "                                  columns=[f'Prediction_{i+1}' for i in range(n_iter)])\n",
    "    df_predictions.insert(0, 'Time', time_points)\n",
    "\n",
    "    \n",
    "    return df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdbc08b6-b096-47d7-a97c-b056072cea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 61.48241329193115 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "time_points = bayes_pred_uncer['Time']\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 실행할 코드\n",
    "bayes_ppd_data = save_bayesian_visual_predictions_to_csv(model, \n",
    "                                                         X_test_scaled,\n",
    "                                                         time_points, \n",
    "                                                         scaler_y,\n",
    "                                                         n_iter=300)\n",
    "\n",
    "# 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 경과 시간 계산\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27a88495-4bbf-4ac6-8f10-d7b356f8aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ppd_data = bayes_ppd_data.assign(Time=time_points.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec73d6e-2b5b-47df-8550-c14f943f9e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d41fd-70c3-433e-b04d-1a8aaadd6543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e5b4747d-54d3-4ec0-8dfc-94acee18de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# @tf.function\n",
    "def predict_and_save_bayesian_visual_predictions(model, \n",
    "                                                 x, \n",
    "                                                 time_points, \n",
    "                                                 scaler_y, \n",
    "                                                 n_iter=5000):\n",
    "    \"\"\"\n",
    "    Bayesian LSTM 모델의 예측 결과를 여러 번 수행하여 불확실성과 신뢰 구간을 포함한\n",
    "    예측 분포를 계산하고 이를 DataFrame으로 반환하는 함수.\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 학습된 Bayesian LSTM 모델\n",
    "    - x: 테스트 데이터의 특성 (정규화된 상태)\n",
    "    - time_points: 시간대 포인트 (원래 시간 데이터)\n",
    "    - scaler_y: 타겟 변수의 스케일러\n",
    "    - n_iter: 예측 수행 횟수\n",
    "    \n",
    "    반환값:\n",
    "    - df_predictions: 예측 결과와 신뢰 구간을 포함한 DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # 여러 번의 예측 수행\n",
    "    result = tf.TensorArray(tf.float32, size=n_iter)\n",
    "    for i in tf.range(n_iter):\n",
    "        result = result.write(i, model(x, training=True))\n",
    "    result = result.stack()  # (n_iter, batch_size, output_size)\n",
    "    \n",
    "    # 평균 예측 값 계산\n",
    "    prediction = tf.reduce_mean(result, axis=0)\n",
    "    \n",
    "    # 불확실성 계산 (표준편차)\n",
    "    uncertainty = tf.math.reduce_std(result, axis=0)\n",
    "    \n",
    "    # 95% 신뢰구간 (credible interval) 계산\n",
    "    lower_bound = tfp.stats.percentile(result, 2.5, axis=0)\n",
    "    upper_bound = tfp.stats.percentile(result, 97.5, axis=0)\n",
    "    \n",
    "    # 예측 결과 역정규화\n",
    "    prediction = scaler_y.inverse_transform(prediction.numpy())\n",
    "    lower_bound = scaler_y.inverse_transform(lower_bound.numpy())\n",
    "    upper_bound = scaler_y.inverse_transform(upper_bound.numpy())\n",
    "    \n",
    "    # 각 예측을 역정규화하고 DataFrame에 포함시키기 위해 loop\n",
    "    predictions_scaled = np.zeros((len(time_points), n_iter))\n",
    "    for j in range(n_iter):\n",
    "        predictions_scaled[:, j] = scaler_y.inverse_transform(result[j].numpy().flatten().reshape(-1, 1)).flatten()\n",
    "\n",
    "    # 시간대와 예측 결과를 DataFrame으로 변환\n",
    "    df_predictions = pd.DataFrame(predictions_scaled, \n",
    "                                  columns=[f'Prediction_{i+1}' for i in range(n_iter)])\n",
    "    df_predictions.insert(0, 'Time', time_points)\n",
    "    df_predictions['Lower_Bound'] = lower_bound.flatten()\n",
    "    df_predictions['Upper_Bound'] = upper_bound.flatten()\n",
    "    df_predictions['Mean_Prediction'] = prediction.flatten()\n",
    "    df_predictions['Uncertainty'] = uncertainty.numpy().flatten()\n",
    "\n",
    "    \n",
    "    # 열 순서 변경: Mean_Prediction, Uncertainty, Lower_Bound, Upper_Bound, Time + 나머지 Prediction 열\n",
    "    cols = ['Time','Mean_Prediction', 'Uncertainty', 'Lower_Bound', 'Upper_Bound'] + [col for col in df_predictions.columns if col.startswith('Prediction_')]\n",
    "    df_predictions = df_predictions[cols]\n",
    "    \n",
    "    return df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e37a7-931d-4eb2-a07d-eb84b1c0cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f2cf83f9350>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/idhkdni/.local/lib/python3.7/site-packages/keras/backend.py\", line 5134, in <genexpr>\n",
      "    for ta, out in zip(output_ta_t, flat_output)  File \"/home/idhkdni/.local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "# 예측 및 불확실성 계산, Bayesian PPD 데이터 생성\n",
    "time_points = test_data[\"Time\"] + pd.Timedelta(hours=3)\n",
    "bayes_ppd_data = predict_and_save_bayesian_visual_predictions(model, \n",
    "                                                              X_test_scaled, \n",
    "                                                              time_points, \n",
    "                                                              scaler_y, \n",
    "                                                              n_iter=300)\n",
    "\n",
    "# # 예측 데이터 정리 및 저장\n",
    "# bayes_pred_uncer = pd.DataFrame({\n",
    "#     'Time': time_points,\n",
    "#     \"True_Value\": test_data['MHC_Water_Level'].shift(-3),\n",
    "#     \"Prediction\": bayes_ppd_data['Mean_Prediction'],\n",
    "#     \"Uncertainty\": bayes_ppd_data['Uncertainty']\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03f79e-f289-4418-909a-c81c6b8dbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ppd_data = bayes_ppd_data.assign(Time=time_points.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2016f-0281-4ad1-be35-283e2708cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ppd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2487fd3a-f270-4150-ae42-8a4a55e0ce53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Prediction_1</th>\n",
       "      <th>Prediction_2</th>\n",
       "      <th>Prediction_3</th>\n",
       "      <th>Prediction_4</th>\n",
       "      <th>Prediction_5</th>\n",
       "      <th>Prediction_6</th>\n",
       "      <th>Prediction_7</th>\n",
       "      <th>Prediction_8</th>\n",
       "      <th>Prediction_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Prediction_291</th>\n",
       "      <th>Prediction_292</th>\n",
       "      <th>Prediction_293</th>\n",
       "      <th>Prediction_294</th>\n",
       "      <th>Prediction_295</th>\n",
       "      <th>Prediction_296</th>\n",
       "      <th>Prediction_297</th>\n",
       "      <th>Prediction_298</th>\n",
       "      <th>Prediction_299</th>\n",
       "      <th>Prediction_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-11 22:00:00</td>\n",
       "      <td>1.526106</td>\n",
       "      <td>1.522710</td>\n",
       "      <td>1.523642</td>\n",
       "      <td>1.523435</td>\n",
       "      <td>1.523199</td>\n",
       "      <td>1.523363</td>\n",
       "      <td>1.520638</td>\n",
       "      <td>1.518787</td>\n",
       "      <td>1.527128</td>\n",
       "      <td>...</td>\n",
       "      <td>1.521467</td>\n",
       "      <td>1.521581</td>\n",
       "      <td>1.531349</td>\n",
       "      <td>1.521500</td>\n",
       "      <td>1.519252</td>\n",
       "      <td>1.521794</td>\n",
       "      <td>1.522144</td>\n",
       "      <td>1.521108</td>\n",
       "      <td>1.523878</td>\n",
       "      <td>1.523028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-11 23:00:00</td>\n",
       "      <td>1.539325</td>\n",
       "      <td>1.535879</td>\n",
       "      <td>1.536786</td>\n",
       "      <td>1.536638</td>\n",
       "      <td>1.536485</td>\n",
       "      <td>1.536480</td>\n",
       "      <td>1.533793</td>\n",
       "      <td>1.532016</td>\n",
       "      <td>1.540433</td>\n",
       "      <td>...</td>\n",
       "      <td>1.534840</td>\n",
       "      <td>1.534775</td>\n",
       "      <td>1.544807</td>\n",
       "      <td>1.534592</td>\n",
       "      <td>1.532485</td>\n",
       "      <td>1.535127</td>\n",
       "      <td>1.535393</td>\n",
       "      <td>1.534505</td>\n",
       "      <td>1.536927</td>\n",
       "      <td>1.536253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-12 00:00:00</td>\n",
       "      <td>1.552055</td>\n",
       "      <td>1.548527</td>\n",
       "      <td>1.549371</td>\n",
       "      <td>1.549372</td>\n",
       "      <td>1.549243</td>\n",
       "      <td>1.549096</td>\n",
       "      <td>1.546424</td>\n",
       "      <td>1.544828</td>\n",
       "      <td>1.553193</td>\n",
       "      <td>...</td>\n",
       "      <td>1.547707</td>\n",
       "      <td>1.547452</td>\n",
       "      <td>1.557702</td>\n",
       "      <td>1.547342</td>\n",
       "      <td>1.545184</td>\n",
       "      <td>1.547853</td>\n",
       "      <td>1.548098</td>\n",
       "      <td>1.547339</td>\n",
       "      <td>1.549518</td>\n",
       "      <td>1.549069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time  Prediction_1  Prediction_2  Prediction_3  \\\n",
       "0  2024-08-11 22:00:00      1.526106      1.522710      1.523642   \n",
       "1  2024-08-11 23:00:00      1.539325      1.535879      1.536786   \n",
       "2  2024-08-12 00:00:00      1.552055      1.548527      1.549371   \n",
       "\n",
       "   Prediction_4  Prediction_5  Prediction_6  Prediction_7  Prediction_8  \\\n",
       "0      1.523435      1.523199      1.523363      1.520638      1.518787   \n",
       "1      1.536638      1.536485      1.536480      1.533793      1.532016   \n",
       "2      1.549372      1.549243      1.549096      1.546424      1.544828   \n",
       "\n",
       "   Prediction_9  ...  Prediction_291  Prediction_292  Prediction_293  \\\n",
       "0      1.527128  ...        1.521467        1.521581        1.531349   \n",
       "1      1.540433  ...        1.534840        1.534775        1.544807   \n",
       "2      1.553193  ...        1.547707        1.547452        1.557702   \n",
       "\n",
       "   Prediction_294  Prediction_295  Prediction_296  Prediction_297  \\\n",
       "0        1.521500        1.519252        1.521794        1.522144   \n",
       "1        1.534592        1.532485        1.535127        1.535393   \n",
       "2        1.547342        1.545184        1.547853        1.548098   \n",
       "\n",
       "   Prediction_298  Prediction_299  Prediction_300  \n",
       "0        1.521108        1.523878        1.523028  \n",
       "1        1.534505        1.536927        1.536253  \n",
       "2        1.547339        1.549518        1.549069  \n",
       "\n",
       "[3 rows x 301 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4e744-b757-4a7e-9b5c-44e51e21b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CSV 파일로 저장\n",
    "# bayes_pred_uncer.to_csv('../streamlit/data/bayes_pred_uncer.csv', index=False)\n",
    "# bayes_ppd_data.to_csv('../streamlit/data/bayesian_ppd_visual.csv', index=False)\n",
    "\n",
    "# print('Bayesian LSTM Model Learning & Prediction Finish\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skku_dmlab [Python/conda:env]",
   "language": "python",
   "name": "conda-env-skku_dmlab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
