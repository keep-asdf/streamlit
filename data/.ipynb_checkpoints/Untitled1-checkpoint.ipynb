{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4590da-45e6-49e2-ab9a-1c2a6fda9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86cbd563-1efd-4be7-9aff-e94c5ea5fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 커스텀 BayesianLSTM 셀 구현\n",
    "class BayesianLSTMCell(tf.keras.layers.LSTMCell):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BayesianLSTMCell, self).__init__(units, **kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn()\n",
    "        self.kernel_prior_fn = lambda dtype, shape, name, trainable, add_variable_fn: tfp.layers.default_multivariate_normal_fn(\n",
    "            dtype=dtype, shape=shape, name=name, trainable=trainable, add_variable_fn=add_variable_fn)\n",
    "        self.recurrent_kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn()\n",
    "        self.recurrent_kernel_prior_fn = lambda dtype, shape, name, trainable, add_variable_fn: tfp.layers.default_multivariate_normal_fn(\n",
    "            dtype=dtype, shape=shape, name=name, trainable=trainable, add_variable_fn=add_variable_fn)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_posterior = self.kernel_posterior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[input_shape[-1], self.units * 4],\n",
    "            name='kernel_posterior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.kernel_prior = self.kernel_prior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[input_shape[-1], self.units * 4],\n",
    "            name='kernel_prior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.recurrent_kernel_posterior = self.recurrent_kernel_posterior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self.units, self.units * 4],\n",
    "            name='recurrent_kernel_posterior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        self.recurrent_kernel_prior = self.recurrent_kernel_prior_fn(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self.units, self.units * 4],\n",
    "            name='recurrent_kernel_prior',\n",
    "            trainable=True,\n",
    "            add_variable_fn=self.add_weight\n",
    "        )\n",
    "        super(BayesianLSTMCell, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        self.kernel = self.kernel_posterior.sample()\n",
    "        self.recurrent_kernel = self.recurrent_kernel_posterior.sample()\n",
    "        return super(BayesianLSTMCell, self).call(inputs, states, training=training)\n",
    "\n",
    "def build_bayesian_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        tf.keras.layers.RNN(BayesianLSTMCell(10)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    # model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    # return model\n",
    "    kl_divergence = sum(model.losses)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                  loss=lambda y_true, \n",
    "                  y_pred: tf.keras.losses.mean_squared_error(y_true, y_pred) + kl_divergence)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c40d8a-34c2-4690-86f9-289a18522a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_prepare_data(data_file):\n",
    "    \"\"\"\n",
    "    주어진 CSV 파일을 불러와 기본 전처리를 수행.\n",
    "    \n",
    "    매개변수:\n",
    "    - data_file: CSV 파일의 경로\n",
    "    \n",
    "    Returns:\n",
    "    - data: 전처리된 데이터프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "    # Prepare the data\n",
    "    data['Time'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "\n",
    "    data['Target_MHC_Water_Level'] = data['MHC_Water_Level'].shift(-3)\n",
    "\n",
    "    # Fill NaN values in 'Target_MHC_Water_Level' with the last value of 'MHC_Water_Level'\n",
    "    data['Target_MHC_Water_Level'].fillna(data['MHC_Water_Level'].iloc[-1], inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Add lag variables for MHC_Water_Level from t-1 to t-10\n",
    "    # for i in range(3, 11):\n",
    "    #     data[f'MHC_Water_Level_lag_{i}'] = data['MHC_Water_Level'].shift(i)\n",
    "    # data.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "    #Add lag variables for MHC_Water_Level from t-1 to t-10\n",
    "    for i in range(3, 6):\n",
    "        data[f'MHC_Water_Level_lag_{i}'] = data['MHC_Water_Level'].shift(i)\n",
    "        data[f'MH_Water_Level_lag_{i}'] = data['MH_Water_Level'].shift(i)\n",
    "        data[f'PG_Water_Level_lag_{i}'] = data['PG_Water_Level'].shift(i)\n",
    "        data[f'HH_Water_Level_lag_{i}'] = data['HH_Water_Level'].shift(i)\n",
    "        data[f'GG_Water_Level_lag_{i}'] = data['GG_Water_Level'].shift(i)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_data(data):\n",
    "    \"\"\"\n",
    "    데이터를 학습 및 테스트 데이터로 분할.\n",
    "    \n",
    "    매개변수:\n",
    "    - data: 전체 데이터프레임\n",
    "    \n",
    "    Returns:\n",
    "    - train_data: 학습 데이터\n",
    "    - test_data: 테스트 데이터\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    test_data = data.iloc[train_size:]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ed5aaa-0325-4907-8577-4fdf3b2d5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bayes_prepare_train_test_sets(train_data, test_data):\n",
    "    X_train = train_data.drop(columns=['Time', 'Target_MHC_Water_Level']).values\n",
    "    y_train = train_data['Target_MHC_Water_Level'].values\n",
    "    X_test = test_data.drop(columns=['Time', 'Target_MHC_Water_Level']).values\n",
    "    y_test = test_data['Target_MHC_Water_Level'].values\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f323e3eb-2caa-4d6e-bcdb-977438ba76ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 12:15:53.077074: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.077247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.077386: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.081192: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.081273: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.081354: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-08-12 12:15:53.081382: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-08-12 12:15:53.081857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/idhkdni/.local/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "501/501 [==============================] - 6s 10ms/step - loss: 9.1897e-04\n",
      "Epoch 2/10\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 6.4493e-04\n",
      "Epoch 3/10\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 6.1359e-04\n",
      "Epoch 4/10\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 5.5442e-04\n",
      "Epoch 5/10\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 5.3998e-04\n",
      "Epoch 6/10\n",
      "501/501 [==============================] - 5s 9ms/step - loss: 5.1617e-04\n",
      "Epoch 7/10\n",
      "501/501 [==============================] - 4s 9ms/step - loss: 5.0629e-04\n",
      "Epoch 8/10\n",
      "501/501 [==============================] - 5s 9ms/step - loss: 4.9381e-04\n",
      "Epoch 9/10\n",
      "501/501 [==============================] - 5s 10ms/step - loss: 4.7908e-04\n",
      "Epoch 10/10\n",
      "501/501 [==============================] - 5s 9ms/step - loss: 4.2047e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff988366410>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "                data = load_and_prepare_data('water_data.csv')\n",
    "                train_data, test_data = split_data(data)\n",
    "                X_train, y_train, X_test, y_test = bayes_prepare_train_test_sets(train_data, test_data)\n",
    "                \n",
    "                \n",
    "                scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "                scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "                X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "                X_test_scaled = scaler_X.transform(X_test)\n",
    "                y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "                \n",
    "                model = build_bayesian_lstm_model(input_shape=(X_train_scaled.shape[1], 1))\n",
    "                X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "                X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "                model.fit(X_train_scaled, y_train_scaled, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519bdae-fff6-4481-b1d7-ee74b11eb4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fe604-0bbc-4a84-be24-294020beda14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811164a5-c79c-4a82-8db1-b1410ffd72b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f2beac-e86e-4c67-ac2a-25d58c9546da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_pred_uncer =pd.read_csv('bayes_pred_uncer.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "936fb9ac-9787-450c-84c6-e56dc6603df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bayesian_visual_predictions_to_csv(model, \n",
    "                                            x,\n",
    "                                            time_points, \n",
    "                                            scaler_y,\n",
    "                                            n_iter=100):\n",
    "    \"\"\"\n",
    "    예측 결과를 CSV 파일로 저장하는 함수\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 학습된 Bayesian LSTM 모델\n",
    "    - x: 테스트 데이터의 특성 (정규화된 상태)\n",
    "    - time_points: 시간대 포인트 (원래 시간 데이터)\n",
    "    - scaler_y: 타겟 변수의 스케일러\n",
    "    - n_iter: 예측 수행 횟수\n",
    "    - filename: 저장할 CSV 파일 이름\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # time_points = time_points[-3:]\n",
    "    # print(time_points)\n",
    "    \n",
    "    predictions_scaled = np.zeros((len(time_points), n_iter))\n",
    "    \n",
    "    for j in range(n_iter):\n",
    "        predictions_scaled[:, j] = model(x, training=True).numpy().flatten()\n",
    "    \n",
    "    # 예측 결과 역정규화\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    # 시간대와 예측 결과를 DataFrame으로 변환\n",
    "    df_predictions = pd.DataFrame(\n",
    "                                  predictions, \n",
    "                                  columns=[f'Prediction_{i+1}' for i in range(n_iter)])\n",
    "    df_predictions.insert(0, 'Time', time_points)\n",
    "\n",
    "    \n",
    "    return df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdbc08b6-b096-47d7-a97c-b056072cea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 61.48241329193115 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "time_points = bayes_pred_uncer['Time']\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 실행할 코드\n",
    "bayes_ppd_data = save_bayesian_visual_predictions_to_csv(model, \n",
    "                                                         X_test_scaled,\n",
    "                                                         time_points, \n",
    "                                                         scaler_y,\n",
    "                                                         n_iter=300)\n",
    "\n",
    "# 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 경과 시간 계산\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27a88495-4bbf-4ac6-8f10-d7b356f8aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ppd_data = bayes_ppd_data.assign(Time=time_points.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec73d6e-2b5b-47df-8550-c14f943f9e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d41fd-70c3-433e-b04d-1a8aaadd6543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5b4747d-54d3-4ec0-8dfc-94acee18de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# @tf.function\n",
    "def predict_and_save_bayesian_visual_predictions(model, \n",
    "                                                 x, \n",
    "                                                 time_points, \n",
    "                                                 scaler_y, \n",
    "                                                 true_value,\n",
    "                                                 n_iter=5000):\n",
    "    \"\"\"\n",
    "    Bayesian LSTM 모델의 예측 결과를 여러 번 수행하여 불확실성과 신뢰 구간을 포함한\n",
    "    예측 분포를 계산하고 이를 DataFrame으로 반환하는 함수.\n",
    "    \n",
    "    매개변수:\n",
    "    - model: 학습된 Bayesian LSTM 모델\n",
    "    - x: 테스트 데이터의 특성 (정규화된 상태)\n",
    "    - time_points: 시간대 포인트 (원래 시간 데이터)\n",
    "    - scaler_y: 타겟 변수의 스케일러\n",
    "    - n_iter: 예측 수행 횟수\n",
    "    \n",
    "    반환값:\n",
    "    - df_predictions: 예측 결과와 신뢰 구간을 포함한 DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # 여러 번의 예측 수행\n",
    "    result = tf.TensorArray(tf.float32, size=n_iter)\n",
    "    for i in tf.range(n_iter):\n",
    "        result = result.write(i, model(x, training=True))\n",
    "    result = result.stack()  # (n_iter, batch_size, output_size)\n",
    "    \n",
    "    # 평균 예측 값 계산\n",
    "    prediction = tf.reduce_mean(result, axis=0)\n",
    "    \n",
    "    # 불확실성 계산 (표준편차)\n",
    "    uncertainty = tf.math.reduce_std(result, axis=0)\n",
    "    \n",
    "    # 95% 신뢰구간 (credible interval) 계산\n",
    "    lower_bound = tfp.stats.percentile(result, 2.5, axis=0)\n",
    "    upper_bound = tfp.stats.percentile(result, 97.5, axis=0)\n",
    "    \n",
    "    # 예측 결과 역정규화\n",
    "    prediction = scaler_y.inverse_transform(prediction.numpy())\n",
    "    lower_bound = scaler_y.inverse_transform(lower_bound.numpy())\n",
    "    upper_bound = scaler_y.inverse_transform(upper_bound.numpy())\n",
    "    \n",
    "    # 각 예측을 역정규화하고 DataFrame에 포함시키기 위해 loop\n",
    "    predictions_scaled = np.zeros((len(time_points), n_iter))\n",
    "    for j in range(n_iter):\n",
    "        predictions_scaled[:, j] = scaler_y.inverse_transform(result[j].numpy().flatten().reshape(-1, 1)).flatten()\n",
    "\n",
    "    # 시간대와 예측 결과를 DataFrame으로 변환\n",
    "    df_predictions = pd.DataFrame(predictions_scaled, \n",
    "                                  columns=[f'Prediction_{i+1}' for i in range(n_iter)])\n",
    "    df_predictions.insert(0, 'Time', time_points)\n",
    "    df_predictions['Lower_Bound'] = lower_bound.flatten()\n",
    "    df_predictions['Upper_Bound'] = upper_bound.flatten()\n",
    "    df_predictions['Mean_Prediction'] = prediction.flatten()\n",
    "    df_predictions['Uncertainty'] = uncertainty.numpy().flatten()\n",
    "    df_predictions['True_value'] = true_value\n",
    "\n",
    "    \n",
    "    # 열 순서 변경: Mean_Prediction, Uncertainty, Lower_Bound, Upper_Bound, Time + 나머지 Prediction 열\n",
    "    cols = ['Time', 'True_value', 'Mean_Prediction', 'Uncertainty', 'Lower_Bound', 'Upper_Bound'] + [col for col in df_predictions.columns if col.startswith('Prediction_')]\n",
    "    df_predictions = df_predictions[cols]\n",
    "    \n",
    "    return df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a7cf66-54a1-43d5-8f7f-89e36bda4388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16022    1.50\n",
       "16023    1.49\n",
       "16024    1.50\n",
       "16025    1.49\n",
       "16026    1.49\n",
       "         ... \n",
       "20022    1.51\n",
       "20023    1.50\n",
       "20024    1.50\n",
       "20025    1.50\n",
       "20026    1.49\n",
       "Name: MHC_Water_Level, Length: 4005, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['MHC_Water_Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a8e37a7-931d-4eb2-a07d-eb84b1c0cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 불확실성 계산, Bayesian PPD 데이터 생성\n",
    "time_points = test_data[\"Time\"] + pd.Timedelta(hours=3)\n",
    "bayes_ppd_data = predict_and_save_bayesian_visual_predictions(model, \n",
    "                                                              X_test_scaled, \n",
    "                                                              time_points,\n",
    "                                                              scaler_y,\n",
    "                                                              # true_value = test_data['MHC_Water_Level'], \n",
    "                                                              n_iter=50)\n",
    "\n",
    "# # 예측 데이터 정리 및 저장\n",
    "# bayes_pred_uncer = pd.DataFrame({\n",
    "#     'Time': time_points,\n",
    "#     \"True_Value\": test_data['MHC_Water_Level'].shift(-3),\n",
    "#     \"Prediction\": bayes_ppd_data['Mean_Prediction'],\n",
    "#     \"Uncertainty\": bayes_ppd_data['Uncertainty']\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c03f79e-f289-4418-909a-c81c6b8dbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ppd_data = bayes_ppd_data.assign(Time=time_points.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c2016f-0281-4ad1-be35-283e2708cfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Mean_Prediction</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Lower_Bound</th>\n",
       "      <th>Upper_Bound</th>\n",
       "      <th>Prediction_1</th>\n",
       "      <th>Prediction_2</th>\n",
       "      <th>Prediction_3</th>\n",
       "      <th>Prediction_4</th>\n",
       "      <th>Prediction_5</th>\n",
       "      <th>...</th>\n",
       "      <th>Prediction_41</th>\n",
       "      <th>Prediction_42</th>\n",
       "      <th>Prediction_43</th>\n",
       "      <th>Prediction_44</th>\n",
       "      <th>Prediction_45</th>\n",
       "      <th>Prediction_46</th>\n",
       "      <th>Prediction_47</th>\n",
       "      <th>Prediction_48</th>\n",
       "      <th>Prediction_49</th>\n",
       "      <th>Prediction_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-27 18:00:00</td>\n",
       "      <td>1.538655</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>1.514522</td>\n",
       "      <td>1.564837</td>\n",
       "      <td>1.561734</td>\n",
       "      <td>1.560965</td>\n",
       "      <td>1.539598</td>\n",
       "      <td>1.523686</td>\n",
       "      <td>1.540223</td>\n",
       "      <td>...</td>\n",
       "      <td>1.544753</td>\n",
       "      <td>1.520570</td>\n",
       "      <td>1.560267</td>\n",
       "      <td>1.520646</td>\n",
       "      <td>1.549334</td>\n",
       "      <td>1.521666</td>\n",
       "      <td>1.543660</td>\n",
       "      <td>1.535542</td>\n",
       "      <td>1.524243</td>\n",
       "      <td>1.520331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-27 19:00:00</td>\n",
       "      <td>1.531852</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>1.507713</td>\n",
       "      <td>1.557985</td>\n",
       "      <td>1.554882</td>\n",
       "      <td>1.554369</td>\n",
       "      <td>1.532951</td>\n",
       "      <td>1.516842</td>\n",
       "      <td>1.533418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537768</td>\n",
       "      <td>1.513883</td>\n",
       "      <td>1.553423</td>\n",
       "      <td>1.513901</td>\n",
       "      <td>1.542416</td>\n",
       "      <td>1.514757</td>\n",
       "      <td>1.536861</td>\n",
       "      <td>1.528666</td>\n",
       "      <td>1.517352</td>\n",
       "      <td>1.513580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-27 20:00:00</td>\n",
       "      <td>1.532189</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>1.507949</td>\n",
       "      <td>1.558395</td>\n",
       "      <td>1.555318</td>\n",
       "      <td>1.554549</td>\n",
       "      <td>1.533150</td>\n",
       "      <td>1.517219</td>\n",
       "      <td>1.533839</td>\n",
       "      <td>...</td>\n",
       "      <td>1.538197</td>\n",
       "      <td>1.514169</td>\n",
       "      <td>1.553789</td>\n",
       "      <td>1.514261</td>\n",
       "      <td>1.542825</td>\n",
       "      <td>1.515106</td>\n",
       "      <td>1.537159</td>\n",
       "      <td>1.529060</td>\n",
       "      <td>1.517823</td>\n",
       "      <td>1.513758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-27 21:00:00</td>\n",
       "      <td>1.529958</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>1.505636</td>\n",
       "      <td>1.556122</td>\n",
       "      <td>1.553142</td>\n",
       "      <td>1.552327</td>\n",
       "      <td>1.530916</td>\n",
       "      <td>1.514979</td>\n",
       "      <td>1.531645</td>\n",
       "      <td>...</td>\n",
       "      <td>1.535936</td>\n",
       "      <td>1.511957</td>\n",
       "      <td>1.551532</td>\n",
       "      <td>1.512063</td>\n",
       "      <td>1.540624</td>\n",
       "      <td>1.512861</td>\n",
       "      <td>1.534943</td>\n",
       "      <td>1.526857</td>\n",
       "      <td>1.515546</td>\n",
       "      <td>1.511516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-27 22:00:00</td>\n",
       "      <td>1.529198</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>1.504930</td>\n",
       "      <td>1.555406</td>\n",
       "      <td>1.552385</td>\n",
       "      <td>1.551578</td>\n",
       "      <td>1.530155</td>\n",
       "      <td>1.514184</td>\n",
       "      <td>1.530884</td>\n",
       "      <td>...</td>\n",
       "      <td>1.535199</td>\n",
       "      <td>1.511189</td>\n",
       "      <td>1.550806</td>\n",
       "      <td>1.511306</td>\n",
       "      <td>1.539843</td>\n",
       "      <td>1.512095</td>\n",
       "      <td>1.534160</td>\n",
       "      <td>1.526049</td>\n",
       "      <td>1.514859</td>\n",
       "      <td>1.510716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>2024-08-12 10:00:00</td>\n",
       "      <td>1.379095</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>1.350863</td>\n",
       "      <td>1.403237</td>\n",
       "      <td>1.401853</td>\n",
       "      <td>1.389992</td>\n",
       "      <td>1.374061</td>\n",
       "      <td>1.363255</td>\n",
       "      <td>1.384079</td>\n",
       "      <td>...</td>\n",
       "      <td>1.380759</td>\n",
       "      <td>1.362036</td>\n",
       "      <td>1.397719</td>\n",
       "      <td>1.383117</td>\n",
       "      <td>1.386615</td>\n",
       "      <td>1.367756</td>\n",
       "      <td>1.384974</td>\n",
       "      <td>1.373151</td>\n",
       "      <td>1.372735</td>\n",
       "      <td>1.363863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>2024-08-12 11:00:00</td>\n",
       "      <td>1.377228</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>1.348909</td>\n",
       "      <td>1.401379</td>\n",
       "      <td>1.400023</td>\n",
       "      <td>1.388170</td>\n",
       "      <td>1.372154</td>\n",
       "      <td>1.361411</td>\n",
       "      <td>1.382249</td>\n",
       "      <td>...</td>\n",
       "      <td>1.378847</td>\n",
       "      <td>1.360209</td>\n",
       "      <td>1.395856</td>\n",
       "      <td>1.381284</td>\n",
       "      <td>1.384706</td>\n",
       "      <td>1.365914</td>\n",
       "      <td>1.383103</td>\n",
       "      <td>1.371323</td>\n",
       "      <td>1.370803</td>\n",
       "      <td>1.361995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>2024-08-12 12:00:00</td>\n",
       "      <td>1.377502</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>1.349223</td>\n",
       "      <td>1.401635</td>\n",
       "      <td>1.400277</td>\n",
       "      <td>1.388383</td>\n",
       "      <td>1.372456</td>\n",
       "      <td>1.361690</td>\n",
       "      <td>1.382509</td>\n",
       "      <td>...</td>\n",
       "      <td>1.379203</td>\n",
       "      <td>1.360388</td>\n",
       "      <td>1.396121</td>\n",
       "      <td>1.381549</td>\n",
       "      <td>1.385026</td>\n",
       "      <td>1.366188</td>\n",
       "      <td>1.383413</td>\n",
       "      <td>1.371519</td>\n",
       "      <td>1.371034</td>\n",
       "      <td>1.362301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>2024-08-12 13:00:00</td>\n",
       "      <td>1.376140</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>1.347851</td>\n",
       "      <td>1.400322</td>\n",
       "      <td>1.398862</td>\n",
       "      <td>1.387080</td>\n",
       "      <td>1.371112</td>\n",
       "      <td>1.360341</td>\n",
       "      <td>1.381177</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377731</td>\n",
       "      <td>1.359096</td>\n",
       "      <td>1.394772</td>\n",
       "      <td>1.380229</td>\n",
       "      <td>1.383674</td>\n",
       "      <td>1.364816</td>\n",
       "      <td>1.382009</td>\n",
       "      <td>1.370085</td>\n",
       "      <td>1.369700</td>\n",
       "      <td>1.360955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>2024-08-12 14:00:00</td>\n",
       "      <td>1.369106</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>1.340727</td>\n",
       "      <td>1.393340</td>\n",
       "      <td>1.391872</td>\n",
       "      <td>1.380166</td>\n",
       "      <td>1.364056</td>\n",
       "      <td>1.353338</td>\n",
       "      <td>1.374221</td>\n",
       "      <td>...</td>\n",
       "      <td>1.370525</td>\n",
       "      <td>1.352068</td>\n",
       "      <td>1.387736</td>\n",
       "      <td>1.373329</td>\n",
       "      <td>1.376574</td>\n",
       "      <td>1.357833</td>\n",
       "      <td>1.374895</td>\n",
       "      <td>1.362944</td>\n",
       "      <td>1.362639</td>\n",
       "      <td>1.353917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4005 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time  Mean_Prediction  Uncertainty  Lower_Bound  \\\n",
       "0    2024-02-27 18:00:00         1.538655     0.001561     1.514522   \n",
       "1    2024-02-27 19:00:00         1.531852     0.001561     1.507713   \n",
       "2    2024-02-27 20:00:00         1.532189     0.001563     1.507949   \n",
       "3    2024-02-27 21:00:00         1.529958     0.001563     1.505636   \n",
       "4    2024-02-27 22:00:00         1.529198     0.001564     1.504930   \n",
       "...                  ...              ...          ...          ...   \n",
       "4000 2024-08-12 10:00:00         1.379095     0.001356     1.350863   \n",
       "4001 2024-08-12 11:00:00         1.377228     0.001356     1.348909   \n",
       "4002 2024-08-12 12:00:00         1.377502     0.001355     1.349223   \n",
       "4003 2024-08-12 13:00:00         1.376140     0.001355     1.347851   \n",
       "4004 2024-08-12 14:00:00         1.369106     0.001356     1.340727   \n",
       "\n",
       "      Upper_Bound  Prediction_1  Prediction_2  Prediction_3  Prediction_4  \\\n",
       "0        1.564837      1.561734      1.560965      1.539598      1.523686   \n",
       "1        1.557985      1.554882      1.554369      1.532951      1.516842   \n",
       "2        1.558395      1.555318      1.554549      1.533150      1.517219   \n",
       "3        1.556122      1.553142      1.552327      1.530916      1.514979   \n",
       "4        1.555406      1.552385      1.551578      1.530155      1.514184   \n",
       "...           ...           ...           ...           ...           ...   \n",
       "4000     1.403237      1.401853      1.389992      1.374061      1.363255   \n",
       "4001     1.401379      1.400023      1.388170      1.372154      1.361411   \n",
       "4002     1.401635      1.400277      1.388383      1.372456      1.361690   \n",
       "4003     1.400322      1.398862      1.387080      1.371112      1.360341   \n",
       "4004     1.393340      1.391872      1.380166      1.364056      1.353338   \n",
       "\n",
       "      Prediction_5  ...  Prediction_41  Prediction_42  Prediction_43  \\\n",
       "0         1.540223  ...       1.544753       1.520570       1.560267   \n",
       "1         1.533418  ...       1.537768       1.513883       1.553423   \n",
       "2         1.533839  ...       1.538197       1.514169       1.553789   \n",
       "3         1.531645  ...       1.535936       1.511957       1.551532   \n",
       "4         1.530884  ...       1.535199       1.511189       1.550806   \n",
       "...            ...  ...            ...            ...            ...   \n",
       "4000      1.384079  ...       1.380759       1.362036       1.397719   \n",
       "4001      1.382249  ...       1.378847       1.360209       1.395856   \n",
       "4002      1.382509  ...       1.379203       1.360388       1.396121   \n",
       "4003      1.381177  ...       1.377731       1.359096       1.394772   \n",
       "4004      1.374221  ...       1.370525       1.352068       1.387736   \n",
       "\n",
       "      Prediction_44  Prediction_45  Prediction_46  Prediction_47  \\\n",
       "0          1.520646       1.549334       1.521666       1.543660   \n",
       "1          1.513901       1.542416       1.514757       1.536861   \n",
       "2          1.514261       1.542825       1.515106       1.537159   \n",
       "3          1.512063       1.540624       1.512861       1.534943   \n",
       "4          1.511306       1.539843       1.512095       1.534160   \n",
       "...             ...            ...            ...            ...   \n",
       "4000       1.383117       1.386615       1.367756       1.384974   \n",
       "4001       1.381284       1.384706       1.365914       1.383103   \n",
       "4002       1.381549       1.385026       1.366188       1.383413   \n",
       "4003       1.380229       1.383674       1.364816       1.382009   \n",
       "4004       1.373329       1.376574       1.357833       1.374895   \n",
       "\n",
       "      Prediction_48  Prediction_49  Prediction_50  \n",
       "0          1.535542       1.524243       1.520331  \n",
       "1          1.528666       1.517352       1.513580  \n",
       "2          1.529060       1.517823       1.513758  \n",
       "3          1.526857       1.515546       1.511516  \n",
       "4          1.526049       1.514859       1.510716  \n",
       "...             ...            ...            ...  \n",
       "4000       1.373151       1.372735       1.363863  \n",
       "4001       1.371323       1.370803       1.361995  \n",
       "4002       1.371519       1.371034       1.362301  \n",
       "4003       1.370085       1.369700       1.360955  \n",
       "4004       1.362944       1.362639       1.353917  \n",
       "\n",
       "[4005 rows x 55 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_ppd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4e744-b757-4a7e-9b5c-44e51e21b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CSV 파일로 저장\n",
    "# bayes_pred_uncer.to_csv('../streamlit/data/bayes_pred_uncer.csv', index=False)\n",
    "# bayes_ppd_data.to_csv('../streamlit/data/bayesian_ppd_visual.csv', index=False)\n",
    "\n",
    "# print('Bayesian LSTM Model Learning & Prediction Finish\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skku_dmlab [Python/conda:env]",
   "language": "python",
   "name": "conda-env-skku_dmlab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
